{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c77cdf8",
   "metadata": {},
   "source": [
    "# 思路1--数据集切分（不叠加）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dd0962",
   "metadata": {},
   "source": [
    "# 思路1 法1 普通求占比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca10666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 定义MAPE函数\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# 加载和预处理数据\n",
    "file_path = r'F:\\8--BJTU大三\\大三下\\商务智能 选修\\团队作业\\ubaar-competition\\train50k_processed.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df = df.iloc[:5000]\n",
    "\n",
    "# 处理'price'列的异常值\n",
    "q99 = df['price'].quantile(0.99)\n",
    "df = df[df['price'] <= q99]\n",
    "\n",
    "# 拆分数据\n",
    "X = df.drop(columns=['price'])\n",
    "y = df['price']\n",
    "\n",
    "# 计算'taxiDurationMin'的四分位数\n",
    "quartiles = df['taxiDurationMin'].quantile([0.25, 0.5, 0.75])\n",
    "\n",
    "# 按四分位数拆分数据集\n",
    "X_0_25 = X[X['taxiDurationMin'] <= quartiles[0.25]]\n",
    "y_0_25 = y[X['taxiDurationMin'] <= quartiles[0.25]]\n",
    "\n",
    "X_25_50 = X[(X['taxiDurationMin'] > quartiles[0.25]) & (X['taxiDurationMin'] <= quartiles[0.5])]\n",
    "y_25_50 = y[(X['taxiDurationMin'] > quartiles[0.25]) & (X['taxiDurationMin'] <= quartiles[0.5])]\n",
    "\n",
    "X_50_75 = X[(X['taxiDurationMin'] > quartiles[0.5]) & (X['taxiDurationMin'] <= quartiles[0.75])]\n",
    "y_50_75 = y[(X['taxiDurationMin'] > quartiles[0.5]) & (X['taxiDurationMin'] <= quartiles[0.75])]\n",
    "\n",
    "X_75_100 = X[X['taxiDurationMin'] > quartiles[0.75]]\n",
    "y_75_100 = y[X['taxiDurationMin'] > quartiles[0.75]]\n",
    "\n",
    "# 训练模型并计算整体数据集的Shapley值\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X, y)\n",
    "\n",
    "explainer = shap.Explainer(model, X)\n",
    "overall_shap_values = explainer(X, check_additivity=False)\n",
    "\n",
    "# 计算每个数据子集的Shapley值\n",
    "sample_groups = [(X_0_25, y_0_25), (X_25_50, y_25_50), (X_50_75, y_50_75), (X_75_100, y_75_100)]\n",
    "sample_shap_values = []\n",
    "\n",
    "for X_sample, y_sample in sample_groups:\n",
    "    explainer_sample = shap.Explainer(model, X_sample)\n",
    "    sample_shap_values.append(explainer_sample(X_sample, check_additivity=False))\n",
    "\n",
    "# 计算数据贡献率\n",
    "# 使用Shapley值计算每个样本的贡献\n",
    "overall_mean_contribution = np.mean(np.abs(overall_shap_values.values), axis=0)\n",
    "\n",
    "# 计算每个样本集的贡献率\n",
    "epsilon = 1e-10  # 小常数，避免除以零\n",
    "contribution_rates = []\n",
    "for shap_values in sample_shap_values:\n",
    "    sample_mean_contribution = np.mean(np.abs(shap_values.values), axis=0)\n",
    "    contribution_rate = np.mean(sample_mean_contribution / (overall_mean_contribution + epsilon))\n",
    "    contribution_rates.append(contribution_rate)\n",
    "\n",
    "# 输出贡献率\n",
    "for i, rate in enumerate(contribution_rates):\n",
    "    print(f'Sample group {i + 1} contribution rate: {rate}')\n",
    "\n",
    "# 绘制每个数据子集的Shapley值图\n",
    "subset_titles = ['0-25th Percentile', '25-50th Percentile', '50-75th Percentile', '75-100th Percentile']\n",
    "for i, shap_values in enumerate(sample_shap_values):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    shap.summary_plot(shap_values, sample_groups[i][0])\n",
    "    plt.title(f'Sample group {i + 1} SHAP values ({subset_titles[i]})')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fc2b27",
   "metadata": {},
   "source": [
    "# 思路1 法2 合作博弈论shapley思想"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84be7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 定义MAPE函数\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# 加载和预处理数据\n",
    "file_path = r'F:\\8--BJTU大三\\大三下\\商务智能 选修\\团队作业\\ubaar-competition\\train50k_processed.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df = df.iloc[:5000]\n",
    "\n",
    "# 处理'price'列的异常值\n",
    "q99 = df['price'].quantile(0.99)\n",
    "df = df[df['price'] <= q99]\n",
    "\n",
    "# 拆分数据\n",
    "X = df.drop(columns=['price'])\n",
    "y = df['price']\n",
    "\n",
    "# 计算'taxiDurationMin'的四分位数\n",
    "quartiles = df['taxiDurationMin'].quantile([0.25, 0.5, 0.75])\n",
    "\n",
    "# 按四分位数拆分数据集\n",
    "X_0_25 = X[X['taxiDurationMin'] <= quartiles[0.25]]\n",
    "y_0_25 = y[X['taxiDurationMin'] <= quartiles[0.25]]\n",
    "\n",
    "X_25_50 = X[(X['taxiDurationMin'] > quartiles[0.25]) & (X['taxiDurationMin'] <= quartiles[0.5])]\n",
    "y_25_50 = y[(X['taxiDurationMin'] > quartiles[0.25]) & (X['taxiDurationMin'] <= quartiles[0.5])]\n",
    "\n",
    "X_50_75 = X[(X['taxiDurationMin'] > quartiles[0.5]) & (X['taxiDurationMin'] <= quartiles[0.75])]\n",
    "y_50_75 = y[(X['taxiDurationMin'] > quartiles[0.5]) & (X['taxiDurationMin'] <= quartiles[0.75])]\n",
    "\n",
    "X_75_100 = X[X['taxiDurationMin'] > quartiles[0.75]]\n",
    "y_75_100 = y[X['taxiDurationMin'] > quartiles[0.75]]\n",
    "\n",
    "# 定义子集\n",
    "sample_groups = [(X_0_25, y_0_25, '0-25th Percentile'), \n",
    "                 (X_25_50, y_25_50, '25-50th Percentile'), \n",
    "                 (X_50_75, y_50_75, '50-75th Percentile'), \n",
    "                 (X_75_100, y_75_100, '75-100th Percentile')]\n",
    "\n",
    "# 训练模型并计算整体数据集的MAPE\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "overall_mape = mean_absolute_percentage_error(y, y_pred)\n",
    "\n",
    "# 计算每个子集的边际贡献\n",
    "marginal_contributions = []\n",
    "for X_sample, y_sample, label in sample_groups:\n",
    "    # 移除当前子集\n",
    "    X_reduced = X.drop(X_sample.index)\n",
    "    y_reduced = y.drop(y_sample.index)\n",
    "    \n",
    "    # 训练模型并计算MAPE\n",
    "    model_reduced = RandomForestRegressor()\n",
    "    model_reduced.fit(X_reduced, y_reduced)\n",
    "    y_reduced_pred = model_reduced.predict(X_reduced)\n",
    "    reduced_mape = mean_absolute_percentage_error(y_reduced, y_reduced_pred)\n",
    "    \n",
    "    # 计算边际贡献\n",
    "    marginal_contribution = overall_mape - reduced_mape\n",
    "    marginal_contributions.append((label, marginal_contribution))\n",
    "\n",
    "# 输出边际贡献\n",
    "for label, contribution in marginal_contributions:\n",
    "    print(f'{label} marginal contribution: {contribution}')\n",
    "\n",
    "# 训练模型并计算每个数据子集的Shapley值\n",
    "explainer = shap.Explainer(model, X)\n",
    "overall_shap_values = explainer(X, check_additivity=False)\n",
    "sample_shap_values = []\n",
    "\n",
    "for X_sample, y_sample, label in sample_groups:\n",
    "    explainer_sample = shap.Explainer(model, X_sample)\n",
    "    sample_shap_values.append((X_sample, label, explainer_sample(X_sample, check_additivity=False)))\n",
    "\n",
    "# 绘制每个数据子集的Shapley值图\n",
    "for X_sample, label, shap_values in sample_shap_values:\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    shap.summary_plot(shap_values, X_sample, show=False)\n",
    "    plt.title(f'SHAP values for {label}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a03548",
   "metadata": {},
   "source": [
    "# 以上均为早期错误思想，但可能也有点小对，你们看着理解下，还用不用的上"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10a6b68",
   "metadata": {},
   "source": [
    "# 正确思想"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8bf5736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\31912\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\Users\\31912\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n",
      "  from pandas.core import (\n",
      "C:\\Users\\31912\\AppData\\Local\\Temp\\ipykernel_6356\\1632260225.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "C:\\Users\\31912\\AppData\\Roaming\\Python\\Python39\\site-packages\\matplotlib\\projections\\__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 39>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m X_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([X_train, splits[i][\u001b[38;5;241m0\u001b[39m]])\n\u001b[0;32m     47\u001b[0m y_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([y_train, splits[i][\u001b[38;5;241m1\u001b[39m]])\n\u001b[1;32m---> 49\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[0;32m     52\u001b[0m mape \u001b[38;5;241m=\u001b[39m mean_absolute_percentage_error(y, y_pred)\n",
      "File \u001b[1;32mD:\\anaconda\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:450\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    439\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    442\u001b[0m ]\n\u001b[0;32m    444\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 450\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_joblib_parallel_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mD:\\anaconda\\lib\\site-packages\\sklearn\\utils\\fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\anaconda\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:185\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    183\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 185\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mD:\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:1315\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\n\u001b[0;32m   1279\u001b[0m     \u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, X_idx_sorted\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1280\u001b[0m ):\n\u001b[0;32m   1281\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1282\u001b[0m \n\u001b[0;32m   1283\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1315\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1317\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1318\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_idx_sorted\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_idx_sorted\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mD:\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:420\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    410\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    411\u001b[0m         splitter,\n\u001b[0;32m    412\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    418\u001b[0m     )\n\u001b[1;32m--> 420\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    423\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import permutations, combinations\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_percentage_error, r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 加载和预处理数据\n",
    "file_path = r'F:\\8--BJTU大三\\大三下\\商务智能 选修\\团队作业\\ubaar-competition\\train50k_processed.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df = df.iloc[:5000]\n",
    "\n",
    "# 处理'price'列的异常值\n",
    "q99 = df['price'].quantile(0.99)\n",
    "df = df[df['price'] <= q99]\n",
    "\n",
    "# 拆分数据\n",
    "X = df.drop(columns=['price'])\n",
    "y = df['price']\n",
    "\n",
    "# 将数据集按10%的比例分成10份\n",
    "n_splits = 10\n",
    "split_size = len(X) // n_splits\n",
    "splits = [(X.iloc[i*split_size:(i+1)*split_size], y.iloc[i*split_size:(i+1)*split_size]) for i in range(n_splits)]\n",
    "\n",
    "# 初始模型训练\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X, y)\n",
    "initial_y_pred = model.predict(X)\n",
    "initial_mape = mean_absolute_percentage_error(y, initial_y_pred)\n",
    "initial_r2 = r2_score(y, initial_y_pred)\n",
    "initial_mse = mean_squared_error(y, initial_y_pred)\n",
    "\n",
    "# Shapley值计算\n",
    "shapley_values_mape = np.zeros(n_splits)\n",
    "shapley_values_r2 = np.zeros(n_splits)\n",
    "shapley_values_mse = np.zeros(n_splits)\n",
    "\n",
    "for perm in permutations(range(n_splits)):\n",
    "    X_train = pd.DataFrame()\n",
    "    y_train = pd.Series(dtype=y.dtype)\n",
    "    prev_mape = initial_mape\n",
    "    prev_r2 = initial_r2\n",
    "    prev_mse = initial_mse\n",
    "    for i in perm:\n",
    "        X_train = pd.concat([X_train, splits[i][0]])\n",
    "        y_train = pd.concat([y_train, splits[i][1]])\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X)\n",
    "        \n",
    "        mape = mean_absolute_percentage_error(y, y_pred)\n",
    "        r2 = r2_score(y, y_pred)\n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        \n",
    "        shapley_values_mape[i] += prev_mape - mape\n",
    "        shapley_values_r2[i] += r2 - prev_r2\n",
    "        shapley_values_mse[i] += prev_mse - mse\n",
    "        \n",
    "        prev_mape = mape\n",
    "        prev_r2 = r2\n",
    "        prev_mse = mse\n",
    "\n",
    "shapley_values_mape /= len(list(permutations(range(n_splits))))\n",
    "shapley_values_r2 /= len(list(permutations(range(n_splits))))\n",
    "shapley_values_mse /= len(list(permutations(range(n_splits))))\n",
    "\n",
    "# 输出Shapley值\n",
    "for i in range(n_splits):\n",
    "    print(f'Shapley value for subset {i+1}:')\n",
    "    print(f'ΔMAPE: {shapley_values_mape[i]}')\n",
    "    print(f'ΔR2: {shapley_values_r2[i]}')\n",
    "    print(f'ΔMSE: {shapley_values_mse[i]}')\n",
    "\n",
    "# 绘制变化曲线图\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(range(1, n_splits+1), shapley_values_mape, marker='o')\n",
    "plt.title('Shapley ΔMAPE')\n",
    "plt.xlabel('Subset')\n",
    "plt.ylabel('ΔMAPE')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(range(1, n_splits+1), shapley_values_r2, marker='o')\n",
    "plt.title('Shapley ΔR2')\n",
    "plt.xlabel('Subset')\n",
    "plt.ylabel('ΔR2')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(range(1, n_splits+1), shapley_values_mse, marker='o')\n",
    "plt.title('Shapley ΔMSE')\n",
    "plt.xlabel('Subset')\n",
    "plt.ylabel('ΔMSE')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
